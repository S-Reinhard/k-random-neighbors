{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e202c5",
   "metadata": {},
   "source": [
    "# K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "76d8a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.extmath import weighted_mode\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml\n",
    "import cProfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8656f1",
   "metadata": {},
   "source": [
    "## K-Random-Neighbors klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "03c8c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRandomNeighbors:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fit_map = None\n",
    "        self.stddev = None\n",
    "        self.X_shape = None\n",
    "        self.XTrain_view = None\n",
    "        self.stddev_target_scaler = 0.5\n",
    "        \n",
    "    def scaled_sigmoid(self, x, a, b, c):\n",
    "        return a / (1 + np.exp(c * (x - b)))\n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain):\n",
    "        # create a hashmap with feature tuples as keys and target values as values\n",
    "        self.fit_map = {tuple(x): y for x, y in zip(XTrain, YTrain)}\n",
    "        \n",
    "        # compute the standard deviation for each column in XTrain\n",
    "        self.stddev = np.std(XTrain, axis=0)\n",
    "        \n",
    "        # store the shape of the training data\n",
    "        self.X_shape = XTrain.shape\n",
    "        \n",
    "        # convert the training data of shape n*m into an ndarray of shape n*1 with tuple-like elements of form 1*m\n",
    "        self.XTrain_view = XTrain.view([('', XTrain.dtype)] * XTrain.shape[1])\n",
    "        \n",
    "        # TODO: optimize stddev_target_scaler\n",
    "        \n",
    "    def predict(self, X, k, num_points=1000):\n",
    "        hits = list()\n",
    "        \n",
    "        while len(hits) < k:\n",
    "            # generate data points\n",
    "            rng = np.random.default_rng()\n",
    "            points: np.ndarray = rng.normal(X, self.stddev * self.stddev_target_scaler, size=(num_points, self.X_shape[1])).round(1)\n",
    "            \n",
    "            # convert points into structured arrays (treat rows as tuples)\n",
    "            point_view = points.view([('', points.dtype)] * points.shape[1])\n",
    "            \n",
    "            # determine the intersection between generated points and training data\n",
    "            intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
    "            print(points[0, :], end='\\r', flush=True)\n",
    "            \n",
    "            if intersection.shape[0] > 0:\n",
    "                hits.extend(intersection.tolist())\n",
    "                \n",
    "        # determine class for each hit\n",
    "        classes = list()\n",
    "        for hit in hits:\n",
    "            classes.append(self.fit_map[hit])\n",
    "            \n",
    "        classes = np.array(classes)\n",
    "        \n",
    "        # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
    "        weights = np.ones(shape=classes.shape)\n",
    "        \n",
    "        # determine the mode (most frequently occurring class)\n",
    "        prediction = weighted_mode(classes, weights)\n",
    "        \n",
    "        return prediction[0][0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3833c",
   "metadata": {},
   "source": [
    "\n",
    "## Daten Aufbereitung (Iris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "2f8f2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data from the file\n",
    "df = pd.read_csv('iris.data', header=None)\n",
    "\n",
    "# Encode plant names as numbers\n",
    "le = LabelEncoder()\n",
    "df[4] = le.fit_transform(df[4])\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('iris_numeric.csv', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ace359",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0fa8c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Features (pixel values of the images)\n",
    "X = digits.data\n",
    "\n",
    "# Labels (the digits)\n",
    "y = digits.target\n",
    "\n",
    "# Combine the data and labels into a DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('digits_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fcfe6b",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cd9511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from OpenML\n",
    "diabetes = fetch_openml(data_id=37, as_frame=True)  # data_id=37 for Pima Indian Diabetes\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = diabetes.frame\n",
    "\n",
    "# Encode classes as numbers\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "\n",
    "# Remove or round columns with inconvenient decimal places\n",
    "df.pop(\"pedi\")\n",
    "df[\"mass\"] = df[\"mass\"].round(0).astype(int)\n",
    "\n",
    "df.pop(\"skin\")\n",
    "df.pop(\"preg\")\n",
    "df.pop(\"pres\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('diabetes_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b1156",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Banknoten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c0bc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "banknote = fetch_openml(data_id=1462, as_frame=True)  # Banknote dataset ID\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = banknote.frame\n",
    "\n",
    "# Round to a uniform number of decimal places\n",
    "df[\"V1\"] = df[\"V1\"].round(1)\n",
    "df[\"V2\"] = df[\"V2\"].round(1)\n",
    "df[\"V3\"] = df[\"V3\"].round(1)\n",
    "df[\"V4\"] = df[\"V4\"].round(1)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('banknotes_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406c9cc",
   "metadata": {},
   "source": [
    "## Test funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "72ecb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(XTrain: np.ndarray, YTrain: np.ndarray, XTest: np.ndarray, YTest: np.ndarray, k=30):\n",
    "    \"\"\"\n",
    "    Tests a k-NN model and prints confusion matrix as well as precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    - XTrain, YTrain: Training data\n",
    "    - XTest, YTest: Test data\n",
    "    - k: Number of neighbors\n",
    "    \"\"\"\n",
    "    # 1) Train the KNN\n",
    "    knn = KRandomNeighbors()\n",
    "    knn.fit(XTrain, YTrain)\n",
    "    \n",
    "    # Time measurement: Total time\n",
    "    start_total = time.time()\n",
    "\n",
    "    # 2) Predictions for all test samples\n",
    "    predictions = []\n",
    "    total_prediction_time = 0\n",
    "\n",
    "    for i in range(XTest.shape[0]):\n",
    "        start_pred = time.time()\n",
    "        pred = knn.predict(XTest[i], k)\n",
    "        end_pred = time.time()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        pred_time = end_pred - start_pred\n",
    "        total_prediction_time += pred_time\n",
    "        print(f\"Predicted class {pred} for {XTest[i]} in {round(pred_time, 3)} sec\")\n",
    "    \n",
    "    end_total = time.time()\n",
    "\n",
    "    # get all classes\n",
    "    classes = np.unique(np.concatenate([YTrain, YTest]))\n",
    "\n",
    "    # 3) Confusion matrix\n",
    "    cm = confusion_matrix(YTest, predictions, labels=classes)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # 4) Precision, Recall, F1-score\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        YTest, predictions, labels=classes, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\nMetrics per class:\")\n",
    "    for idx, label in enumerate(classes):\n",
    "        print(f\"Class {label}:\")\n",
    "        print(f\"  Precision: {precision[idx]:.2f}\")\n",
    "        print(f\"  Recall:    {recall[idx]:.2f}\")\n",
    "        print(f\"  F1-Score:  {f1[idx]:.2f}\")\n",
    "        print(f\"  Support:   {support[idx]} samples\\n\")\n",
    "        \n",
    "    # Average time per prediction\n",
    "    avg_prediction_time = total_prediction_time / XTest.shape[0]\n",
    "\n",
    "    # Time per feature\n",
    "    num_features = XTest.shape[1]\n",
    "    avg_time_per_feature = avg_prediction_time / num_features\n",
    "\n",
    "    print(\"Total time:\", round(end_total - start_total, 3), \"seconds\")\n",
    "    print(\"Avg. time per prediction:\", round(avg_prediction_time, 3), \"seconds\")\n",
    "    print(\"Avg. time per feature:\", round(avg_time_per_feature, 3), \"seconds\")\n",
    "\n",
    "    return cm, precision, recall, f1, support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ad92e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2bed15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2742acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9 3.  1.4 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "Predicted class 0 for [4.9 3.  1.4 0.2] in 0.042 sec\n",
      "Predicted class 0 for [4.4 2.9 1.4 0.2] in 0.075 sec\n",
      "Predicted class 0 for [5.4 3.7 1.5 0.2] in 0.016 sec\n",
      "Predicted class 0 for [4.8 3.  1.4 0.1] in 0.061 sec\n",
      "Predicted class 0 for [5.4 3.9 1.3 0.4] in 0.017 sec\n",
      "Predicted class 0 for [5.1 3.3 1.7 0.5] in 0.025 sec\n",
      "Predicted class 0 for [5.2 3.4 1.4 0.2] in 0.02 sec\n",
      "Predicted class 0 for [4.7 3.2 1.6 0.2] in 0.02 sec\n",
      "Predicted class 0 for [4.9 3.1 1.5 0.1] in 0.034 sec\n",
      "Predicted class 0 for [4.4 3.  1.3 0.2] in 0.054 sec\n",
      "Predicted class 0 for [5.  3.5 1.6 0.6] in 0.039 sec\n",
      "Predicted class 0 for [4.8 3.  1.4 0.3] in 0.054 sec\n",
      "Predicted class 0 for [5.  3.3 1.4 0.2] in 0.009 sec\n",
      "Predicted class 1 for [5.  2.  3.5 1. ] in 0.179 sec\n",
      "Predicted class 1 for [6.1 2.9 4.7 1.4] in 0.013 sec\n",
      "Predicted class 1 for [5.6 2.9 3.6 1.3] in 0.021 sec\n",
      "Predicted class 1 for [5.9 3.2 4.8 1.8] in 0.032 sec\n",
      "Predicted class 1 for [6.1 2.8 4.  1.3] in 0.013 sec\n",
      "Predicted class 1 for [6.8 2.8 4.8 1.4] in 0.034 sec\n",
      "Predicted class 2 for [6.7 3.  5.  1.7] in 0.022 sec\n",
      "Predicted class 1 for [6.  2.9 4.5 1.5] in 0.017 sec\n",
      "Predicted class 1 for [5.7 2.9 4.2 1.3] in 0.021 sec\n",
      "Predicted class 2 for [6.9 3.2 5.7 2.3] in 0.04 sec\n",
      "Predicted class 2 for [6.7 3.3 5.7 2.1] in 0.036 sec\n",
      "Predicted class 2 for [6.3 2.8 5.1 1.5] in 0.014 sec\n",
      "Predicted class 2 for [6.  3.  4.8 1.8] in 0.02 sec\n",
      "Predicted class 2 for [6.9 3.1 5.1 2.3] in 0.048 sec\n",
      "Predicted class 2 for [5.8 2.7 5.1 1.9] in 0.023 sec\n",
      "Predicted class 2 for [6.5 3.  5.2 2. ] in 0.023 sec\n",
      "Predicted class 2 for [5.9 3.  5.1 1.8] in 0.028 sec\n",
      "Confusion Matrix:\n",
      " [[13  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0  8]]\n",
      "\n",
      "Metrics per class:\n",
      "Class 0.0:\n",
      "  Precision: 1.00\n",
      "  Recall:    1.00\n",
      "  F1-Score:  1.00\n",
      "  Support:   13 samples\n",
      "\n",
      "Class 1.0:\n",
      "  Precision: 1.00\n",
      "  Recall:    0.89\n",
      "  F1-Score:  0.94\n",
      "  Support:   9 samples\n",
      "\n",
      "Class 2.0:\n",
      "  Precision: 0.89\n",
      "  Recall:    1.00\n",
      "  F1-Score:  0.94\n",
      "  Support:   8 samples\n",
      "\n",
      "Total time: 1.047 seconds\n",
      "Avg. time per prediction: 0.035 seconds\n",
      "Avg. time per feature: 0.009 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[13,  0,  0],\n",
       "        [ 0,  8,  1],\n",
       "        [ 0,  0,  8]]),\n",
       " array([1.        , 1.        , 0.88888889]),\n",
       " array([1.        , 0.88888889, 1.        ]),\n",
       " array([1.        , 0.94117647, 0.94117647]),\n",
       " array([13,  9,  8]))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the numeric data as an ndarray\n",
    "dataset = np.loadtxt('iris_numeric.csv', delimiter=',')\n",
    "# dataset = np.loadtxt('banknotes_numeric.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "allIdx = np.arange(X.shape[0])  # all indices of the data\n",
    "\n",
    "# randomly choose 80% of the indices for training\n",
    "rng = np.random.default_rng(42)\n",
    "train_idx = rng.choice(allIdx, size=int(allIdx.shape[0] * 0.8), replace=False)\n",
    "test_idx = np.delete(allIdx, train_idx)  # remaining 20% for testing\n",
    "\n",
    "XTrain = X[train_idx]\n",
    "YTrain = Y[train_idx]\n",
    "\n",
    "XTest = X[test_idx]\n",
    "YTest = Y[test_idx]\n",
    "print(XTest)\n",
    "\n",
    "# test the k-NN model with k=3\n",
    "test_knn(XTrain, YTrain, XTest, YTest, k=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3685eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.9 3.4 1.1 0.5]0. ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.0190352 s\n",
      "File: C:\\Users\\svenr\\AppData\\Local\\Temp\\ipykernel_33568\\193846564.py\n",
      "Function: predict at line 28\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    28                                               def predict(self, X, k, num_points=1000):\n",
      "    29         1         10.0     10.0      0.0          hits = list()\n",
      "    30                                           \n",
      "    31         7         79.0     11.3      0.0          while len(hits) < k:\n",
      "    32                                                       # generate data points\n",
      "    33         6       4198.0    699.7      2.2              rng = np.random.default_rng()\n",
      "    34         6      14271.0   2378.5      7.5              points: np.ndarray = rng.normal(X, self.stddev * self.stddev_target_scaler, size=(num_points, self.X_shape[1])).round(1)\n",
      "    35                                           \n",
      "    36                                                       # convert points into structured arrays (treat rows as tuples)\n",
      "    37         6        909.0    151.5      0.5              point_view = points.view([('', points.dtype)] * points.shape[1])\n",
      "    38                                           \n",
      "    39                                                       # determine the intersection between generated points and training data\n",
      "    40         6     118953.0  19825.5     62.5              intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
      "    41         6      50237.0   8372.8     26.4              print(points[0, :], end='\\r', flush=True)\n",
      "    42                                           \n",
      "    43         6         72.0     12.0      0.0              if intersection.shape[0] > 0:\n",
      "    44         4         93.0     23.2      0.0                  hits.extend(intersection.tolist())\n",
      "    45                                           \n",
      "    46                                                   # determine class for each hit\n",
      "    47         1         12.0     12.0      0.0          classes = list()\n",
      "    48         6         30.0      5.0      0.0          for hit in hits:\n",
      "    49         5         84.0     16.8      0.0              classes.append(self.fit_map[hit])\n",
      "    50                                           \n",
      "    51         1         44.0     44.0      0.0          classes = np.array(classes)\n",
      "    52                                           \n",
      "    53                                                   # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
      "    54         1        158.0    158.0      0.1          weights = np.ones(shape=classes.shape)\n",
      "    55                                           \n",
      "    56                                                   # determine the mode (most frequently occurring class)\n",
      "    57         1       1151.0   1151.0      0.6          prediction = weighted_mode(classes, weights)\n",
      "    58                                           \n",
      "    59         1         51.0     51.0      0.0          return prediction[0][0].astype(int)"
     ]
    }
   ],
   "source": [
    "%lprun -f KRandomNeighbors.predict test_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d08687",
   "metadata": {},
   "source": [
    "### Auswertung Banknotes\n",
    "\n",
    "#### Confusion Matrix:\n",
    "|                          | Vorhergesagt: Klasse 1.0 | Vorhergesagt: Klasse 2.0 |\n",
    "|--------------------------|--------------------------|--------------------------|\n",
    "| Tatsächlich: Klasse 1.0  | 141                      | 11                       |\n",
    "| Tatsächlich: Klasse 2.0  | 0                        | 123                      |\n",
    "\n",
    "#### Metriken pro Klasse:\n",
    "Klasse 1.0:\n",
    "- Präzision: 1.00\n",
    "- Recall:    0.93\n",
    "- F1-Score:  0.96\n",
    "- Support:   152 Beispiele\n",
    "\n",
    "Klasse 2.0:\n",
    "- Präzision: 0.92\n",
    "- Recall:    1.00\n",
    "- F1-Score:  0.96\n",
    "- Support:   123 Beispiele\n",
    "\n",
    "#### Zeitmessungen:\n",
    "- Gesamtzeit: 295.178 Sekunden\n",
    "- Ø Zeit pro Vorhersage: 1.073 Sekunden\n",
    "- Ø Zeit pro Feature: 0.268 Sekunden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952c20c",
   "metadata": {},
   "source": [
    "# Auswertung Iris\n",
    "\n",
    "## Konfusionsmatrix\n",
    "\n",
    "|               | Vorhergesagt: Klasse 0 | Vorhergesagt: Klasse 1 | Vorhergesagt: Klasse 2 |\n",
    "|---------------|------------------------|------------------------|------------------------|\n",
    "| **Tatsächliche Klasse 0** | 11                     | 0                      | 0                      |\n",
    "| **Tatsächliche Klasse 1** | 0                      | 9                      | 2                      |\n",
    "| **Tatsächliche Klasse 2** | 0                      | 2                      | 6                      |\n",
    "\n",
    "\n",
    "\n",
    "## Metriken pro Klasse\n",
    "\n",
    "### Klasse 0.0\n",
    "- **Precision:** 1.00  \n",
    "- **Recall:** 1.00  \n",
    "- **F1-Score:** 1.00  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "### Klasse 1.0\n",
    "- **Precision:** 0.82  \n",
    "- **Recall:** 0.82  \n",
    "- **F1-Score:** 0.82  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "### Klasse 2.0\n",
    "- **Precision:** 0.75  \n",
    "- **Recall:** 0.75  \n",
    "- **F1-Score:** 0.75  \n",
    "- **Support:** 8 Samples  \n",
    "\n",
    "## Zeitmessung\n",
    "\n",
    "- **Gesamtdauer:** 0.409 Sekunden  \n",
    "- **Ø  Zeit pro Vorhersage:** 0.014 Sekunden  \n",
    "- **Ø  Zeit pro Feature:** 0.003 Sekunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa9af6",
   "metadata": {},
   "source": [
    "# Idee für bessere Skalierbarkeit mit Dimensionen\n",
    "\n",
    "Divide and Conquer: Wenn mehr als 4 Features, daten in mehrere Tabellen aufteilen und in jeder Tabelle getrennt von einander Schießen. Anschließend die Trefferklassen kombinieren. ACHTUNG: wirkt sich auch auf potentiellen Bias aus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c021bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
