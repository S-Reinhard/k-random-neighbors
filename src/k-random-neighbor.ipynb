{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e202c5",
   "metadata": {},
   "source": [
    "# KNN - Monte-Carlo Ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.extmath import weighted_mode\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3833c",
   "metadata": {},
   "source": [
    "\n",
    "## Daten Aufbereitung (Iris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data from the file\n",
    "df = pd.read_csv('iris.data', header=None)\n",
    "\n",
    "# Encode plant names as numbers\n",
    "le = LabelEncoder()\n",
    "df[4] = le.fit_transform(df[4])\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('iris_numeric.csv', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ace359",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa8c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Features (pixel values of the images)\n",
    "X = digits.data\n",
    "\n",
    "# Labels (the digits)\n",
    "y = digits.target\n",
    "\n",
    "# Combine the data and labels into a DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('digits_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fcfe6b",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from OpenML\n",
    "diabetes = fetch_openml(data_id=37, as_frame=True)  # data_id=37 for Pima Indian Diabetes\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = diabetes.frame\n",
    "\n",
    "# Encode classes as numbers\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "\n",
    "# Remove or round columns with inconvenient decimal places\n",
    "df.pop(\"pedi\")\n",
    "df[\"mass\"] = df[\"mass\"].round(0).astype(int)\n",
    "\n",
    "df.pop(\"skin\")\n",
    "df.pop(\"preg\")\n",
    "df.pop(\"pres\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('diabetes_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b1156",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Banknoten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "banknote = fetch_openml(data_id=1462, as_frame=True)  # Banknote dataset ID\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = banknote.frame\n",
    "\n",
    "# Round to a uniform number of decimal places\n",
    "df[\"V1\"] = df[\"V1\"].round(1)\n",
    "df[\"V2\"] = df[\"V2\"].round(1)\n",
    "df[\"V3\"] = df[\"V3\"].round(1)\n",
    "df[\"V4\"] = df[\"V4\"].round(1)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('banknotes_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8656f1",
   "metadata": {},
   "source": [
    "## K-Monte-Carlo klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNMonteCarlo:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fit_map = None\n",
    "        self.stddev = None\n",
    "        self.X_shape = None\n",
    "        self.XTrain_view = None\n",
    "        self.stddev_target_scaler = 0.5\n",
    "        \n",
    "    def scaled_sigmoid(self, x, a, b, c):\n",
    "        return a / (1 + np.exp(c * (x - b)))\n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain):\n",
    "        # create a hashmap with feature tuples as keys and target values as values\n",
    "        self.fit_map = {tuple(x): y for x, y in zip(XTrain, YTrain)}\n",
    "        \n",
    "        # compute the standard deviation for each column in XTrain\n",
    "        self.stddev = np.std(XTrain, axis=0)\n",
    "        \n",
    "        # store the shape of the training data\n",
    "        self.X_shape = XTrain.shape\n",
    "        \n",
    "        # convert the training data of shape n*m into an ndarray of shape n*1 with tuple-like elements of form 1*m\n",
    "        self.XTrain_view = XTrain.view([('', XTrain.dtype)] * XTrain.shape[1])\n",
    "        \n",
    "        # TODO: optimize stddev_target_scaler\n",
    "        \n",
    "    def predict(self, X, k, num_points=1000):\n",
    "        hits = list()\n",
    "        \n",
    "        while len(hits) < k:\n",
    "            # generate data points\n",
    "            rng = np.random.default_rng()\n",
    "            points: np.ndarray = rng.normal(X, self.stddev * self.stddev_target_scaler, size=(num_points, self.X_shape[1])).round(1)\n",
    "            \n",
    "            # convert points into structured arrays (treat rows as tuples)\n",
    "            point_view = points.view([('', points.dtype)] * points.shape[1])\n",
    "            \n",
    "            # determine the intersection between generated points and training data\n",
    "            intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
    "            print(points[0, :], end='\\r', flush=True)\n",
    "            \n",
    "            if intersection.shape[0] > 0:\n",
    "                hits.extend(intersection.tolist())\n",
    "                \n",
    "        # determine class for each hit\n",
    "        classes = list()\n",
    "        for hit in hits:\n",
    "            classes.append(self.fit_map[hit])\n",
    "            \n",
    "        classes = np.array(classes)\n",
    "        \n",
    "        # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
    "        weights = np.ones(shape=classes.shape)\n",
    "        \n",
    "        # determine the mode (most frequently occurring class)\n",
    "        prediction = weighted_mode(classes, weights)\n",
    "        \n",
    "        return prediction[0][0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406c9cc",
   "metadata": {},
   "source": [
    "## Test funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(XTrain: np.ndarray, YTrain: np.ndarray, XTest: np.ndarray, YTest: np.ndarray, k=30):\n",
    "    \"\"\"\n",
    "    Tests a k-NN model and prints confusion matrix as well as precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    - XTrain, YTrain: Training data\n",
    "    - XTest, YTest: Test data\n",
    "    - k: Number of neighbors\n",
    "    \"\"\"\n",
    "    # 1) Train the KNN\n",
    "    knn = KNNMonteCarlo()\n",
    "    knn.fit(XTrain, YTrain)\n",
    "    \n",
    "    # Time measurement: Total time\n",
    "    start_total = time.time()\n",
    "\n",
    "    # 2) Predictions for all test samples\n",
    "    predictions = []\n",
    "    total_prediction_time = 0\n",
    "\n",
    "    for i in range(XTest.shape[0]):\n",
    "        start_pred = time.time()\n",
    "        pred = knn.predict(XTest[i], k)\n",
    "        end_pred = time.time()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        pred_time = end_pred - start_pred\n",
    "        total_prediction_time += pred_time\n",
    "        print(f\"Predicted class {pred} for {XTest[i]} in {round(pred_time, 3)} sec\")\n",
    "    \n",
    "    end_total = time.time()\n",
    "\n",
    "    # get all classes\n",
    "    classes = np.unique(np.concatenate([YTrain, YTest]))\n",
    "\n",
    "    # 3) Confusion matrix\n",
    "    cm = confusion_matrix(YTest, predictions, labels=classes)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # 4) Precision, Recall, F1-score\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        YTest, predictions, labels=classes, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\nMetrics per class:\")\n",
    "    for idx, label in enumerate(classes):\n",
    "        print(f\"Class {label}:\")\n",
    "        print(f\"  Precision: {precision[idx]:.2f}\")\n",
    "        print(f\"  Recall:    {recall[idx]:.2f}\")\n",
    "        print(f\"  F1-Score:  {f1[idx]:.2f}\")\n",
    "        print(f\"  Support:   {support[idx]} samples\\n\")\n",
    "        \n",
    "    # Average time per prediction\n",
    "    avg_prediction_time = total_prediction_time / XTest.shape[0]\n",
    "\n",
    "    # Time per feature\n",
    "    num_features = XTest.shape[1]\n",
    "    avg_time_per_feature = avg_prediction_time / num_features\n",
    "\n",
    "    print(\"Total time:\", round(end_total - start_total, 3), \"seconds\")\n",
    "    print(\"Avg. time per prediction:\", round(avg_prediction_time, 3), \"seconds\")\n",
    "    print(\"Avg. time per feature:\", round(avg_time_per_feature, 3), \"seconds\")\n",
    "\n",
    "    return cm, precision, recall, f1, support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ad92e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numeric data as an ndarray\n",
    "dataset = np.loadtxt('banknotes_numeric.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "allIdx = np.arange(X.shape[0])  # all indices of the data\n",
    "\n",
    "# randomly choose 80% of the indices for training\n",
    "train_idx = np.random.choice(allIdx, size=int(allIdx.shape[0] * 0.8), replace=False)\n",
    "test_idx = np.delete(allIdx, train_idx)  # remaining 20% for testing\n",
    "\n",
    "XTrain = X[train_idx]\n",
    "YTrain = Y[train_idx]\n",
    "\n",
    "XTest = X[test_idx]\n",
    "YTest = Y[test_idx]\n",
    "print(XTest)\n",
    "\n",
    "# test the k-NN model with k=3\n",
    "test_knn(XTrain, YTrain, XTest, YTest, k=3)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d08687",
   "metadata": {},
   "source": [
    "### Auswertung Banknotes\n",
    "\n",
    "#### Confusion Matrix:\n",
    "|                          | Vorhergesagt: Klasse 1.0 | Vorhergesagt: Klasse 2.0 |\n",
    "|--------------------------|--------------------------|--------------------------|\n",
    "| Tatsächlich: Klasse 1.0  | 141                      | 11                       |\n",
    "| Tatsächlich: Klasse 2.0  | 0                        | 123                      |\n",
    "\n",
    "#### Metriken pro Klasse:\n",
    "Klasse 1.0:\n",
    "- Präzision: 1.00\n",
    "- Recall:    0.93\n",
    "- F1-Score:  0.96\n",
    "- Support:   152 Beispiele\n",
    "\n",
    "Klasse 2.0:\n",
    "- Präzision: 0.92\n",
    "- Recall:    1.00\n",
    "- F1-Score:  0.96\n",
    "- Support:   123 Beispiele\n",
    "\n",
    "#### Zeitmessungen:\n",
    "- Gesamtzeit: 295.178 Sekunden\n",
    "- Ø Zeit pro Vorhersage: 1.073 Sekunden\n",
    "- Ø Zeit pro Feature: 0.268 Sekunden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa9af6",
   "metadata": {},
   "source": [
    "# Idee für bessere Skalierbarkeit mit Dimensionen\n",
    "\n",
    "Divide and Conquer: Wenn mehr als 4 Features, daten in mehrere Tabellen aufteilen und in jeder Tabelle getrennt von einander Schießen. Anschließend die Trefferklassen kombinieren. ACHTUNG: wirkt sich auch auf potentiellen Bias aus.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
