{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e202c5",
   "metadata": {},
   "source": [
    "# K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.extmath import weighted_mode\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8656f1",
   "metadata": {},
   "source": [
    "## K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRandomNeighbors:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fit_map = None\n",
    "        self.stddev = None\n",
    "        self.X_shape = None\n",
    "        self.XTrain_view = None\n",
    "        self.YTrain = None\n",
    "        self.stddev_target_scaler = 0.5\n",
    "        \n",
    "    def scaled_sigmoid(self, x, a, b, c):\n",
    "        return a / (1 + np.exp(c * (x - b)))\n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain: np.ndarray):\n",
    "        self.YTrain = YTrain\n",
    "        \n",
    "        # create a hashmap with feature tuples as keys and target values as values\n",
    "        self.fit_map = {tuple(x): y for x, y in zip(XTrain, YTrain)}\n",
    "        \n",
    "        # compute the standard deviation for each column in XTrain\n",
    "        self.stddev = np.std(XTrain, axis=0)\n",
    "        \n",
    "        # store the shape of the training data\n",
    "        self.X_shape = XTrain.shape\n",
    "        \n",
    "        # convert the training data of shape n*m into an ndarray of shape n*1 with tuple-like elements of form 1*m\n",
    "        XTrain = np.ascontiguousarray(XTrain)\n",
    "        self.XTrain_view = XTrain.view([('', XTrain.dtype)] * XTrain.shape[1])\n",
    "        \n",
    "        # TODO: optimize stddev_target_scaler\n",
    "        \n",
    "    def predict(self, X, k, num_points=1000, print_shots=\"no\"):\n",
    "        hits = list()\n",
    "        \n",
    "        while len(hits) < k:\n",
    "            # generate data points\n",
    "            rng = np.random.default_rng()\n",
    "            shot_shape = (num_points, self.X_shape[1])\n",
    "            points: np.ndarray = rng.normal(X, self.stddev /2 , size=shot_shape).round(1)\n",
    "            \n",
    "            # convert points into structured arrays (treat rows as tuples)\n",
    "            point_view = points.view([('', points.dtype)] * points.shape[1])\n",
    "            \n",
    "            # determine the intersection between generated points and training data\n",
    "            intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
    "            if (print_shots == \"yes\"):\n",
    "                print(points[0, :], end='\\r', flush=True)\n",
    "            \n",
    "            if intersection.shape[0] > 0:\n",
    "                hits.extend(intersection.tolist())\n",
    "                \n",
    "        # determine class for each hit\n",
    "        classes = list()\n",
    "        for hit in hits:\n",
    "            classes.append(self.fit_map[hit])\n",
    "            \n",
    "        classes = np.array(classes)\n",
    "        \n",
    "        # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
    "        weights = np.ones(shape=classes.shape)\n",
    "        \n",
    "        # determine the mode (most frequently occurring class)\n",
    "        prediction = weighted_mode(classes, weights)\n",
    "        \n",
    "        return prediction[0][0].astype(int)\n",
    "    \n",
    "    \n",
    "    def test(self, XTest: np.ndarray, YTest: np.ndarray, k:int = 11) -> tuple[float | np.ndarray, float | np.ndarray, float | np.ndarray, np.ndarray | None]:\n",
    "        predictions = list()\n",
    "        \n",
    "        \n",
    "        for i in range(XTest.shape[0]):\n",
    "            prediction = self.predict(XTest[i], k)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        classes = np.unique(np.concatenate([YTest, self.YTrain]))\n",
    "        return precision_recall_fscore_support(YTest, predictions, labels=classes, zero_division=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7279b",
   "metadata": {},
   "source": [
    "# k-random-enseble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699be2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRandomEnsable:\n",
    "    \n",
    "    def __init__(self, combined_features=4, weight_strategy=\"f1\"):\n",
    "        self.COMBINED_FEATURES = combined_features\n",
    "        self.weight_strategy = weight_strategy  # e.g. \"precision\", \"recall\", \"f1\", \"uniform\"\n",
    "        self.WORKERS:list[tuple[KRandomNeighbors, float, np.ndarray]] = []\n",
    "\n",
    "        \n",
    "    def get_feature_combinations(self, dims):\n",
    "        # make sure that feature combinations are not longer than existing features\n",
    "        if (dims < self.COMBINED_FEATURES):\n",
    "            self.COMBINED_FEATURES = dims\n",
    "        elif (dims == self.COMBINED_FEATURES):\n",
    "            self.COMBINED_FEATURES -= 1\n",
    "        \n",
    "        # get idx of all features\n",
    "        feature_idxs = list(range(dims))\n",
    "        \n",
    "        # get all combinations of Features\n",
    "        return np.array(list(combinations(feature_idxs, self.COMBINED_FEATURES)))\n",
    "    \n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain: np.ndarray, XTest: np.ndarray, YTest: np.ndarray):\n",
    "        # get possible combinations of features\n",
    "        combs = self.get_feature_combinations(XTrain.shape[1])\n",
    "        \n",
    "        # decide how many workers schould be used -> use as many combinations as there are cores, unless there are more cores than combinations\n",
    "        worker_count = min(multiprocessing.cpu_count(), combs.shape[0])\n",
    "        \n",
    "        # choose the combinations on which the workers are trained on\n",
    "        rng = np.random.default_rng()\n",
    "        comb_idxs = rng.choice(combs.shape[0], worker_count, replace=False)\n",
    "        combs = combs[comb_idxs]\n",
    "        \n",
    "        # train each worker with one combination\n",
    "        for current_combo in combs:            \n",
    "            # generate training dataset by using only the features at the given idxs in the current combination\n",
    "            X_subset = XTrain[:, current_combo]\n",
    "            \n",
    "            # create and Train worker\n",
    "            worker:KRandomNeighbors = KRandomNeighbors()\n",
    "            worker.fit(X_subset, YTrain)\n",
    "            \n",
    "            # evaluate trusworthyness of the worker\n",
    "            (precision, recall, fscore, support) = worker.test(XTest[:, current_combo], YTest)\n",
    "            \n",
    "            # weight according to strategie\n",
    "            weight = self.get_weight(precision, recall, fscore)\n",
    "                \n",
    "            self.WORKERS.append((worker, weight, np.array(current_combo)))\n",
    "\n",
    "    def get_weight(self, precision, recall, fscore):\n",
    "        weight = 1\n",
    "        if self.weight_strategy == \"precision\":\n",
    "            weight = np.mean(precision)\n",
    "        elif self.weight_strategy == \"recall\":\n",
    "            weight = np.mean(recall)\n",
    "        elif self.weight_strategy == \"f1\":\n",
    "            weight= np.mean(fscore)\n",
    "        else:  # uniform\n",
    "            weight = 1.0\n",
    "        return weight\n",
    "    \n",
    "    def _predict_worker(self, worker: KRandomNeighbors, weight: float, x_subset: np.ndarray, k: int):\n",
    "        pred = worker.predict(x_subset, k=k)\n",
    "        return pred, weight\n",
    "\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            results = Parallel(n_jobs=-1)(\n",
    "                delayed(self._predict_worker)(worker, weight, X[i][feature_idxs], k=11)\n",
    "                for worker, weight, feature_idxs in self.WORKERS\n",
    "            )\n",
    "            \n",
    "            votes, weights = zip(*results)\n",
    "            votes = np.array(votes)\n",
    "            weights = np.array(weights)\n",
    "            \n",
    "            final_pred = weighted_mode(votes, w=weights)[0][0]\n",
    "            predictions.append(final_pred)\n",
    "            print(f\"Predicted {final_pred} for {X[i]}\")\n",
    "            \n",
    "        return np.array(predictions)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406c9cc",
   "metadata": {},
   "source": [
    "## Test funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_krn(XTrain: np.ndarray, YTrain: np.ndarray, XTest: np.ndarray, YTest: np.ndarray, k=11, do_print = \"none\"):\n",
    "    \"\"\"\n",
    "    Evaluates a KRandomNeighbors model on a given test set and optionally prints detailed metrics.\n",
    "\n",
    "    Trains a KRandomNeighbors model on the provided training set and evaluates it on the test set.\n",
    "    Computes predictions, confusion matrix, precision, recall, F1-score, and timing information.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    XTrain : np.ndarray\n",
    "        Feature matrix for training data.\n",
    "    YTrain : np.ndarray\n",
    "        Labels for training data.\n",
    "    XTest : np.ndarray\n",
    "        Feature matrix for test data.\n",
    "    YTest : np.ndarray\n",
    "        Labels for test data.\n",
    "    k : int, optional (default=11)\n",
    "        Number of nearest neighbors to consider during prediction.\n",
    "    do_print : str, optional (default=\"none\")\n",
    "        Controls output verbosity:\n",
    "        - \"none\": no output\n",
    "        - \"metrics\": print confusion matrix and evaluation metrics\n",
    "        - \"all\": also print individual predictions and their computation time\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    cm : np.ndarray\n",
    "        Confusion matrix.\n",
    "    precision : np.ndarray\n",
    "        Precision score per class.\n",
    "    recall : np.ndarray\n",
    "        Recall score per class.\n",
    "    f1 : np.ndarray\n",
    "        F1-score per class.\n",
    "    support : np.ndarray\n",
    "        Number of true instances per class in the test set.\n",
    "    total_time : float\n",
    "        Total time taken for the entire test run (in seconds).\n",
    "    time_per_prediction : float\n",
    "        Average time per prediction (in seconds).\n",
    "    time_per_feature : float\n",
    "        Average prediction time per feature (in seconds).\n",
    "    \"\"\"\n",
    "    # 1) Train the KNN\n",
    "    krn = KRandomNeighbors()\n",
    "    krn.fit(XTrain, YTrain)\n",
    "    \n",
    "    # Time measurement: Total time\n",
    "    start_total = time.time()\n",
    "\n",
    "    # 2) Predictions for all test samples\n",
    "    predictions = []\n",
    "    total_prediction_time = 0\n",
    "    pshots = \"yes\" if do_print==\"all\" else \"no\"\n",
    "\n",
    "    for i in range(XTest.shape[0]):\n",
    "        start_pred = time.time()\n",
    "        pred = krn.predict(XTest[i], 11, print_shots=pshots)\n",
    "        end_pred = time.time()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        pred_time = end_pred - start_pred\n",
    "        total_prediction_time += pred_time\n",
    "        \n",
    "        if(do_print == \"all\"):\n",
    "            print(f\"Predicted class {pred} for {XTest[i]} in {round(pred_time, 3)} sec\")\n",
    "    \n",
    "    end_total = time.time()\n",
    "\n",
    "    # get all classes\n",
    "    classes = np.unique(np.concatenate([YTrain, YTest]))\n",
    "\n",
    "    # 3) Confusion matrix\n",
    "    cm = confusion_matrix(YTest, predictions, labels=classes)\n",
    "    \n",
    "    if (do_print == \"metrics\" or do_print == \"all\"):\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # 4) Precision, Recall, F1-score\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        YTest, predictions, labels=classes, zero_division=0\n",
    "    )\n",
    "    \n",
    "    if (do_print == \"metrics\" or do_print == \"all\"):\n",
    "        print(\"\\nMetrics per class:\")\n",
    "        for idx, label in enumerate(classes):\n",
    "            print(f\"Class {label}:\")\n",
    "            print(f\"  Precision: {precision[idx]:.2f}\")\n",
    "            print(f\"  Recall:    {recall[idx]:.2f}\")\n",
    "            print(f\"  F1-Score:  {f1[idx]:.2f}\")\n",
    "            print(f\"  Support:   {support[idx]} samples\\n\")\n",
    "        \n",
    "    # Average time per prediction\n",
    "    avg_prediction_time = total_prediction_time / XTest.shape[0]\n",
    "\n",
    "    # Time per feature\n",
    "    num_features = XTest.shape[1]\n",
    "    avg_time_per_feature = avg_prediction_time / num_features\n",
    "\n",
    "    total_time = round(end_total - start_total, 3)\n",
    "    time_per_prediction = round(avg_prediction_time, 3)\n",
    "    time_per_feature = round(avg_time_per_feature, 3)\n",
    "    if (do_print == \"metrics\" or do_print == \"all\"):\n",
    "        print(\"Total time:\", total_time, \"seconds\")\n",
    "        print(\"Avg. time per prediction:\", time_per_prediction, \"seconds\")\n",
    "        print(\"Avg. time per feature:\", time_per_feature, \"seconds\")\n",
    "\n",
    "    return cm, precision, recall, f1, support, total_time, time_per_prediction, time_per_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ad92e",
   "metadata": {},
   "source": [
    "## Test: K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numeric data as an ndarray\n",
    "dataset = np.loadtxt('iris_numeric.csv', delimiter=',')\n",
    "# dataset = np.loadtxt('iris_synthetic.csv', delimiter=',')\n",
    "# dataset = np.loadtxt('banknotes_numeric.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "allIdx = np.arange(X.shape[0])  # all indices of the data\n",
    "\n",
    "# randomly choose 75% of the indices for training\n",
    "rng = np.random.default_rng()\n",
    "tests = dict()\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(60, 61, 10):\n",
    "    train_idx = rng.choice(allIdx, size=int(allIdx.shape[0] * 0.75), replace=False)\n",
    "    # train_idx = rng.choice(allIdx, size=i, replace=False)\n",
    "    test_idx = np.delete(allIdx, train_idx)  # remaining for testing\n",
    "    # test_idx = rng.choice(test_idx, size=100, replace=False)\n",
    "\n",
    "    XTrain = X[train_idx]\n",
    "    YTrain = Y[train_idx]\n",
    "\n",
    "    XTest = X[test_idx]\n",
    "    YTest = Y[test_idx]\n",
    "\n",
    "\n",
    "    # %lprun -f KRandomNeighbors.predict test_knn(XTrain, YTrain, XTest, YTest, k=11)\n",
    "\n",
    "    # Run KNN test and store result\n",
    "    cm, precision, recall, f1, support, total_time, time_per_prediction, time_per_feature = test_krn(\n",
    "        XTrain, YTrain, XTest, YTest, k=11, do_print=\"all\"\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"train_size\": i,\n",
    "        \"total_time\": total_time,\n",
    "        \"time_per_prediction\": time_per_prediction,\n",
    "        \"time_per_feature\": time_per_feature\n",
    "    })\n",
    "\n",
    "    print(f\"Train size: {i}, time(s): {[total_time, time_per_prediction, time_per_feature]}\")\n",
    "\n",
    "# Als DataFrame speichern\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Als CSV exportieren\n",
    "# df_results.to_csv(\"knn_runtime_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c515f498",
   "metadata": {},
   "source": [
    "## Plot time per prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df97b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CSV if not already in memory\n",
    "# df_results = pd.read_csv(\"knn_runtime_results.csv\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each metric\n",
    "# plt.plot(df_results[\"train_size\"], df_results[\"total_time\"], label=\"Total Time (s)\")\n",
    "plt.plot(df_results[\"train_size\"], df_results[\"time_per_prediction\"], label=\"Time per Prediction (s)\")\n",
    "# plt.plot(df_results[\"train_size\"], df_results[\"time_per_feature\"], label=\"Time per Feature (s)\")\n",
    "\n",
    "# Chart settings\n",
    "plt.title(\"Time Measurements vs. Training Size\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.xscale(\"log\")  # For log-scaled x-axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa333428",
   "metadata": {},
   "source": [
    "## Test K-Random-Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_krandomensemble(X: np.ndarray, Y: np.ndarray, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits data, trains a KRandomEnsable model, and prints evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix.\n",
    "    Y : np.ndarray\n",
    "        Target labels.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Step 1: Split data into Train (60%), Eval (20%), Test (20%)\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "        X, Y, train_size=0.6, stratify=Y, random_state=random_state\n",
    "    )\n",
    "    X_eval, X_test, Y_eval, Y_test = train_test_split(\n",
    "        X_temp, Y_temp, test_size=0.5, stratify=Y_temp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Print split sizes\n",
    "    print(f\"Train size: {len(Y_train)}\")\n",
    "    print(f\"Eval size:  {len(Y_eval)}\")\n",
    "    print(f\"Test size:  {len(Y_test)}\")\n",
    "\n",
    "    # Step 2: Train model\n",
    "    model = KRandomEnsable()\n",
    "    model.fit(X_train, Y_train, X_eval, Y_eval)\n",
    "\n",
    "    # Step 3: Predict\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Step 4: Evaluation\n",
    "    classes = np.unique(np.concatenate([Y_train, Y_test]))\n",
    "    cm = confusion_matrix(Y_test, predictions, labels=classes)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        Y_test, predictions, labels=classes, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Step 5: Print results\n",
    "    print(\"\\nðŸ§® Confusion Matrix\")\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Evaluation per class:\")\n",
    "    print(f\"{'Class':>8} | {'Precision':>9} | {'Recall':>6} | {'F1-score':>8} | {'Support':>7}\")\n",
    "    print(\"-\" * 50)\n",
    "    for cls, p, r, f, s in zip(classes, precision, recall, f1, support):\n",
    "        print(f\"{str(cls):>8} | {p:9.3f} | {r:6.3f} | {f:8.3f} | {s:7d}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Avg.':>8} | {np.mean(precision):9.3f} | {np.mean(recall):6.3f} | {np.mean(f1):8.3f} |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba4b47",
   "metadata": {},
   "source": [
    "## Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f15b1",
   "metadata": {},
   "source": [
    "### Predict 6d with K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numeric data as an ndarray\n",
    "dataset = np.loadtxt('synthetic_6d_clusters.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "allIdx = np.arange(X.shape[0])  # all indices of the data\n",
    "\n",
    "# randomly choose 75% of the indices for training\n",
    "rng = np.random.default_rng()\n",
    "train_idx = rng.choice(allIdx, size=int(allIdx.shape[0] * 0.75), replace=False)\n",
    "test_idx = np.delete(allIdx, train_idx)  # remaining for testing\n",
    "\n",
    "XTrain = X[train_idx]\n",
    "YTrain = Y[train_idx]\n",
    "XTest = X[test_idx]\n",
    "YTest = Y[test_idx]\n",
    "\n",
    "# Run KNN test and store result\n",
    "cm, precision, recall, f1, support, total_time, time_per_prediction, time_per_feature = test_krn(\n",
    "    XTrain, YTrain, XTest, YTest, k=11, do_print=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcb999",
   "metadata": {},
   "source": [
    "### Predict 6d with K-Random-Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6de7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('iris_numeric.csv', delimiter=',')\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]\n",
    "\n",
    "evaluate_krandomensemble(X, Y, random_state=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d08687",
   "metadata": {},
   "source": [
    "## Auswertung Banknotes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d43c41",
   "metadata": {},
   "source": [
    "\n",
    "#### Confusion Matrix:\n",
    "|                          | Vorhergesagt: Klasse 1.0 | Vorhergesagt: Klasse 2.0 |\n",
    "|--------------------------|--------------------------|--------------------------|\n",
    "| TatsÃ¤chlich: Klasse 1.0  | 141                      | 11                       |\n",
    "| TatsÃ¤chlich: Klasse 2.0  | 0                        | 123                      |\n",
    "\n",
    "#### Metriken pro Klasse:\n",
    "Klasse 1.0:\n",
    "- PrÃ¤zision: 1.00\n",
    "- Recall:    0.93\n",
    "- F1-Score:  0.96\n",
    "- Support:   152 Beispiele\n",
    "\n",
    "Klasse 2.0:\n",
    "- PrÃ¤zision: 0.92\n",
    "- Recall:    1.00\n",
    "- F1-Score:  0.96\n",
    "- Support:   123 Beispiele\n",
    "\n",
    "#### Zeitmessungen:\n",
    "- Gesamtzeit: 295.178 Sekunden\n",
    "- Ã˜ Zeit pro Vorhersage: 1.073 Sekunden\n",
    "- Ã˜ Zeit pro Feature: 0.268 Sekunden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952c20c",
   "metadata": {},
   "source": [
    "## Auswertung Iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93317d",
   "metadata": {},
   "source": [
    "\n",
    "### Konfusionsmatrix\n",
    "\n",
    "|               | Vorhergesagt: Klasse 0 | Vorhergesagt: Klasse 1 | Vorhergesagt: Klasse 2 |\n",
    "|---------------|------------------------|------------------------|------------------------|\n",
    "| **TatsÃ¤chliche Klasse 0** | 11                     | 0                      | 0                      |\n",
    "| **TatsÃ¤chliche Klasse 1** | 0                      | 9                      | 2                      |\n",
    "| **TatsÃ¤chliche Klasse 2** | 0                      | 2                      | 6                      |\n",
    "\n",
    "\n",
    "\n",
    "### Metriken pro Klasse\n",
    "\n",
    "#### Klasse 0.0\n",
    "- **Precision:** 1.00  \n",
    "- **Recall:** 1.00  \n",
    "- **F1-Score:** 1.00  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "#### Klasse 1.0\n",
    "- **Precision:** 0.82  \n",
    "- **Recall:** 0.82  \n",
    "- **F1-Score:** 0.82  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "#### Klasse 2.0\n",
    "- **Precision:** 0.75  \n",
    "- **Recall:** 0.75  \n",
    "- **F1-Score:** 0.75  \n",
    "- **Support:** 8 Samples  \n",
    "\n",
    "### Zeitmessung\n",
    "\n",
    "- **Gesamtdauer:** 0.409 Sekunden  \n",
    "- **Ã˜  Zeit pro Vorhersage:** 0.014 Sekunden  \n",
    "- **Ã˜  Zeit pro Feature:** 0.003 Sekunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa9af6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcda31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265aaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('iris_numeric.csv', delimiter=',')\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]\n",
    "\n",
    "evaluate_krandomensemble(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numeric data as an ndarray\n",
    "dataset = np.loadtxt('iris_numeric.csv', delimiter=',')\n",
    "# dataset = np.loadtxt('banknotes_numeric.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "# Erst in Train + Temp (Train = 60%, Temp = 40%)\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=0.6, stratify=Y, random_state=42)\n",
    "\n",
    "# Dann Temp in Eval + Test (je 50% von 40% â†’ 20% + 20%)\n",
    "X_eval, X_test, Y_eval, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, stratify=Y_temp, random_state=42)\n",
    "\n",
    "# Kontrollausgabe\n",
    "print(f\"Train size: {len(Y_train)}\")\n",
    "print(f\"Eval size:  {len(Y_eval)}\")\n",
    "print(f\"Test size:  {len(Y_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = KRandomEnsable()\n",
    "\n",
    "g.fit(XTrain, YTrain, XTest, YTest)\n",
    "predictions = g.predict(XTest)\n",
    "\n",
    "# get all classes\n",
    "classes = np.unique(np.concatenate([YTrain, YTest]))\n",
    "\n",
    "# 3) Confusion matrix\n",
    "cm = confusion_matrix(YTest, predictions, labels=classes)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 4) Precision, Recall, F1-score\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    YTest, predictions, labels=classes, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation per class:\")\n",
    "print(f\"{'Class':>8} | {'Precision':>9} | {'Recall':>6} | {'F1-score':>8} | {'Support':>7}\")\n",
    "print(\"-\" * 50)\n",
    "for cls, p, r, f, s in zip(classes, precision, recall, f1, support):\n",
    "    print(f\"{str(cls):>8} | {p:9.3f} | {r:6.3f} | {f:8.3f} | {s:7d}\")\n",
    "\n",
    "# Optional: Gesamtdurchschnitt (macro avg)\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Avg.':>8} | {np.mean(precision):9.3f} | {np.mean(recall):6.3f} | {np.mean(f1):8.3f} |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a27374d",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluation Summary of K-Random-Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36371b",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset Split:**\n",
    "- **Train size:** 90\n",
    "- **Eval size:** 30\n",
    "- **Test size:** 30\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Confusion Matrix\n",
    "| Actual \\ Predicted |   0   |   1   |   2   |\n",
    "|--------------------|-------|-------|-------|\n",
    "| **0**              |  11   |   0   |   0   |\n",
    "| **1**              |   0   |  13   |   0   |\n",
    "| **2**              |   0   |   1   |  13   |\n",
    "\n",
    "\n",
    "### ðŸ“ˆ Evaluation per Class\n",
    "\n",
    "| Class | Precision | Recall | F1-score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| 0.0   | 1.000     | 1.000  | 1.000    | 11      |\n",
    "| 1.0   | 0.929     | 1.000  | 0.963    | 13      |\n",
    "| 2.0   | 1.000     | 0.929  | 0.963    | 14      |\n",
    "\n",
    "**Average:**\n",
    "- Precision: **0.976**\n",
    "- Recall: **0.976**\n",
    "- F1-score: **0.975**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
