{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e202c5",
   "metadata": {},
   "source": [
    "# K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76d8a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.extmath import weighted_mode\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml\n",
    "import cProfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8656f1",
   "metadata": {},
   "source": [
    "## K-Random-Neighbors klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03c8c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRandomNeighbors:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fit_map = None\n",
    "        self.stddev = None\n",
    "        self.X_shape = None\n",
    "        self.XTrain_view = None\n",
    "        self.stddev_target_scaler = 0.5\n",
    "        \n",
    "    def scaled_sigmoid(self, x, a, b, c):\n",
    "        return a / (1 + np.exp(c * (x - b)))\n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain):\n",
    "        # create a hashmap with feature tuples as keys and target values as values\n",
    "        self.fit_map = {tuple(x): y for x, y in zip(XTrain, YTrain)}\n",
    "        \n",
    "        # compute the standard deviation for each column in XTrain\n",
    "        self.stddev = np.std(XTrain, axis=0)\n",
    "        \n",
    "        # store the shape of the training data\n",
    "        self.X_shape = XTrain.shape\n",
    "        \n",
    "        # convert the training data of shape n*m into an ndarray of shape n*1 with tuple-like elements of form 1*m\n",
    "        self.XTrain_view = XTrain.view([('', XTrain.dtype)] * XTrain.shape[1])\n",
    "        \n",
    "        # TODO: optimize stddev_target_scaler\n",
    "        \n",
    "    def predict(self, X, k, num_points=1000):\n",
    "        hits = list()\n",
    "        \n",
    "        while len(hits) < k:\n",
    "            # generate data points\n",
    "            rng = np.random.default_rng()\n",
    "            points: np.ndarray = rng.normal(X, self.stddev * self.stddev_target_scaler, size=(num_points, self.X_shape[1])).round(1)\n",
    "            \n",
    "            # convert points into structured arrays (treat rows as tuples)\n",
    "            point_view = points.view([('', points.dtype)] * points.shape[1])\n",
    "            \n",
    "            # determine the intersection between generated points and training data\n",
    "            intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
    "            print(points[0, :], end='\\r', flush=True)\n",
    "            \n",
    "            if intersection.shape[0] > 0:\n",
    "                hits.extend(intersection.tolist())\n",
    "                \n",
    "        # determine class for each hit\n",
    "        classes = list()\n",
    "        for hit in hits:\n",
    "            classes.append(self.fit_map[hit])\n",
    "            \n",
    "        classes = np.array(classes)\n",
    "        \n",
    "        # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
    "        weights = np.ones(shape=classes.shape)\n",
    "        \n",
    "        # determine the mode (most frequently occurring class)\n",
    "        prediction = weighted_mode(classes, weights)\n",
    "        \n",
    "        return prediction[0][0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f3833c",
   "metadata": {},
   "source": [
    "\n",
    "## Daten Aufbereitung (Iris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f8f2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data from the file\n",
    "df = pd.read_csv('iris.data', header=None)\n",
    "\n",
    "# Encode plant names as numbers\n",
    "le = LabelEncoder()\n",
    "df[4] = le.fit_transform(df[4])\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('iris_numeric.csv', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ace359",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0fa8c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Features (pixel values of the images)\n",
    "X = digits.data\n",
    "\n",
    "# Labels (the digits)\n",
    "y = digits.target\n",
    "\n",
    "# Combine the data and labels into a DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('digits_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fcfe6b",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd9511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from OpenML\n",
    "diabetes = fetch_openml(data_id=37, as_frame=True)  # data_id=37 for Pima Indian Diabetes\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = diabetes.frame\n",
    "\n",
    "# Encode classes as numbers\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "\n",
    "# Remove or round columns with inconvenient decimal places\n",
    "df.pop(\"pedi\")\n",
    "df[\"mass\"] = df[\"mass\"].round(0).astype(int)\n",
    "\n",
    "df.pop(\"skin\")\n",
    "df.pop(\"preg\")\n",
    "df.pop(\"pres\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('diabetes_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b1156",
   "metadata": {},
   "source": [
    "## Datenaufbereitung (Banknoten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0bc738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "banknote = fetch_openml(data_id=1462, as_frame=True)  # Banknote dataset ID\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = banknote.frame\n",
    "\n",
    "# Round to a uniform number of decimal places\n",
    "df[\"V1\"] = df[\"V1\"].round(1)\n",
    "df[\"V2\"] = df[\"V2\"].round(1)\n",
    "df[\"V3\"] = df[\"V3\"].round(1)\n",
    "df[\"V4\"] = df[\"V4\"].round(1)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('banknotes_numeric.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406c9cc",
   "metadata": {},
   "source": [
    "## Test funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72ecb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(XTrain: np.ndarray, YTrain: np.ndarray, XTest: np.ndarray, YTest: np.ndarray, k=30):\n",
    "    \"\"\"\n",
    "    Tests a k-NN model and prints confusion matrix as well as precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    - XTrain, YTrain: Training data\n",
    "    - XTest, YTest: Test data\n",
    "    - k: Number of neighbors\n",
    "    \"\"\"\n",
    "    # 1) Train the KNN\n",
    "    knn = KRandomNeighbors()\n",
    "    knn.fit(XTrain, YTrain)\n",
    "    \n",
    "    # Time measurement: Total time\n",
    "    start_total = time.time()\n",
    "\n",
    "    # 2) Predictions for all test samples\n",
    "    predictions = []\n",
    "    total_prediction_time = 0\n",
    "\n",
    "    for i in range(XTest.shape[0]):\n",
    "        start_pred = time.time()\n",
    "        pred = knn.predict(XTest[i], k)\n",
    "        end_pred = time.time()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        pred_time = end_pred - start_pred\n",
    "        total_prediction_time += pred_time\n",
    "        print(f\"Predicted class {pred} for {XTest[i]} in {round(pred_time, 3)} sec\")\n",
    "    \n",
    "    end_total = time.time()\n",
    "\n",
    "    # get all classes\n",
    "    classes = np.unique(np.concatenate([YTrain, YTest]))\n",
    "\n",
    "    # 3) Confusion matrix\n",
    "    cm = confusion_matrix(YTest, predictions, labels=classes)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # 4) Precision, Recall, F1-score\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        YTest, predictions, labels=classes, zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\nMetrics per class:\")\n",
    "    for idx, label in enumerate(classes):\n",
    "        print(f\"Class {label}:\")\n",
    "        print(f\"  Precision: {precision[idx]:.2f}\")\n",
    "        print(f\"  Recall:    {recall[idx]:.2f}\")\n",
    "        print(f\"  F1-Score:  {f1[idx]:.2f}\")\n",
    "        print(f\"  Support:   {support[idx]} samples\\n\")\n",
    "        \n",
    "    # Average time per prediction\n",
    "    avg_prediction_time = total_prediction_time / XTest.shape[0]\n",
    "\n",
    "    # Time per feature\n",
    "    num_features = XTest.shape[1]\n",
    "    avg_time_per_feature = avg_prediction_time / num_features\n",
    "\n",
    "    print(\"Total time:\", round(end_total - start_total, 3), \"seconds\")\n",
    "    print(\"Avg. time per prediction:\", round(avg_prediction_time, 3), \"seconds\")\n",
    "    print(\"Avg. time per feature:\", round(avg_time_per_feature, 3), \"seconds\")\n",
    "\n",
    "    return cm, precision, recall, f1, support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ad92e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2bed15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2742acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.5   8.2  -2.5  -1.5]\n",
      " [  4.4   9.7  -4.   -3.2]\n",
      " [  4.   -2.7   2.4   0.9]\n",
      " ...\n",
      " [ -1.   -0.2  -0.9   0. ]\n",
      " [ -2.2   1.6   0.   -1.7]\n",
      " [ -3.8 -13.5  17.6  -2.8]]\n"
     ]
    }
   ],
   "source": [
    "# load the numeric data as an ndarray\n",
    "# dataset = np.loadtxt('iris_numeric.csv', delimiter=',')\n",
    "dataset = np.loadtxt('banknotes_numeric.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "allIdx = np.arange(X.shape[0])  # all indices of the data\n",
    "\n",
    "# randomly choose 80% of the indices for training\n",
    "train_idx = np.random.choice(allIdx, size=int(allIdx.shape[0] * 0.8), replace=False)\n",
    "test_idx = np.delete(allIdx, train_idx)  # remaining 20% for testing\n",
    "\n",
    "XTrain = X[train_idx]\n",
    "YTrain = Y[train_idx]\n",
    "\n",
    "XTest = X[test_idx]\n",
    "YTest = Y[test_idx]\n",
    "print(XTest)\n",
    "\n",
    "# test the k-NN model with k=3\n",
    "# test_knn(XTrain, YTrain, XTest, YTest, k=3)\n",
    "\n",
    "def test_predict():\n",
    "    model = KRandomNeighbors()\n",
    "    model.fit(XTrain, YTrain)\n",
    "    model.predict(XTest[0], k = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3685eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.4  5.9 -5.2 -0.9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 1.13775 s\n",
      "File: C:\\Users\\svenr\\AppData\\Local\\Temp\\ipykernel_3604\\193846564.py\n",
      "Function: predict at line 28\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    28                                               def predict(self, X, k, num_points=1000):\n",
      "    29         1          8.0      8.0      0.0          hits = list()\n",
      "    30                                           \n",
      "    31       389       2941.0      7.6      0.0          while len(hits) < k:\n",
      "    32                                                       # generate data points\n",
      "    33       388     229506.0    591.5      2.0              rng = np.random.default_rng()\n",
      "    34       388     733575.0   1890.7      6.4              points: np.ndarray = rng.normal(X, self.stddev * self.stddev_target_scaler, size=(num_points, self.X_shape[1])).round(1)\n",
      "    35                                           \n",
      "    36                                                       # convert points into structured arrays (treat rows as tuples)\n",
      "    37       388      49156.0    126.7      0.4              point_view = points.view([('', points.dtype)] * points.shape[1])\n",
      "    38                                           \n",
      "    39                                                       # determine the intersection between generated points and training data\n",
      "    40       388    8109616.0  20901.1     71.3              intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
      "    41       388    2247036.0   5791.3     19.7              print(points[0, :], end='\\r', flush=True)\n",
      "    42                                           \n",
      "    43       388       4053.0     10.4      0.0              if intersection.shape[0] > 0:\n",
      "    44         5        120.0     24.0      0.0                  hits.extend(intersection.tolist())\n",
      "    45                                           \n",
      "    46                                                   # determine class for each hit\n",
      "    47         1          6.0      6.0      0.0          classes = list()\n",
      "    48         6         20.0      3.3      0.0          for hit in hits:\n",
      "    49         5         85.0     17.0      0.0              classes.append(self.fit_map[hit])\n",
      "    50                                           \n",
      "    51         1         34.0     34.0      0.0          classes = np.array(classes)\n",
      "    52                                           \n",
      "    53                                                   # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
      "    54         1        141.0    141.0      0.0          weights = np.ones(shape=classes.shape)\n",
      "    55                                           \n",
      "    56                                                   # determine the mode (most frequently occurring class)\n",
      "    57         1       1138.0   1138.0      0.0          prediction = weighted_mode(classes, weights)\n",
      "    58                                           \n",
      "    59         1         62.0     62.0      0.0          return prediction[0][0].astype(int)"
     ]
    }
   ],
   "source": [
    "%lprun -f KRandomNeighbors.predict test_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d08687",
   "metadata": {},
   "source": [
    "### Auswertung Banknotes\n",
    "\n",
    "#### Confusion Matrix:\n",
    "|                          | Vorhergesagt: Klasse 1.0 | Vorhergesagt: Klasse 2.0 |\n",
    "|--------------------------|--------------------------|--------------------------|\n",
    "| Tatsächlich: Klasse 1.0  | 141                      | 11                       |\n",
    "| Tatsächlich: Klasse 2.0  | 0                        | 123                      |\n",
    "\n",
    "#### Metriken pro Klasse:\n",
    "Klasse 1.0:\n",
    "- Präzision: 1.00\n",
    "- Recall:    0.93\n",
    "- F1-Score:  0.96\n",
    "- Support:   152 Beispiele\n",
    "\n",
    "Klasse 2.0:\n",
    "- Präzision: 0.92\n",
    "- Recall:    1.00\n",
    "- F1-Score:  0.96\n",
    "- Support:   123 Beispiele\n",
    "\n",
    "#### Zeitmessungen:\n",
    "- Gesamtzeit: 295.178 Sekunden\n",
    "- Ø Zeit pro Vorhersage: 1.073 Sekunden\n",
    "- Ø Zeit pro Feature: 0.268 Sekunden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952c20c",
   "metadata": {},
   "source": [
    "# Auswertung Iris\n",
    "\n",
    "## Konfusionsmatrix\n",
    "\n",
    "|               | Vorhergesagt: Klasse 0 | Vorhergesagt: Klasse 1 | Vorhergesagt: Klasse 2 |\n",
    "|---------------|------------------------|------------------------|------------------------|\n",
    "| **Tatsächliche Klasse 0** | 11                     | 0                      | 0                      |\n",
    "| **Tatsächliche Klasse 1** | 0                      | 9                      | 2                      |\n",
    "| **Tatsächliche Klasse 2** | 0                      | 2                      | 6                      |\n",
    "\n",
    "\n",
    "\n",
    "## Metriken pro Klasse\n",
    "\n",
    "### Klasse 0.0\n",
    "- **Precision:** 1.00  \n",
    "- **Recall:** 1.00  \n",
    "- **F1-Score:** 1.00  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "### Klasse 1.0\n",
    "- **Precision:** 0.82  \n",
    "- **Recall:** 0.82  \n",
    "- **F1-Score:** 0.82  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "### Klasse 2.0\n",
    "- **Precision:** 0.75  \n",
    "- **Recall:** 0.75  \n",
    "- **F1-Score:** 0.75  \n",
    "- **Support:** 8 Samples  \n",
    "\n",
    "## Zeitmessung\n",
    "\n",
    "- **Gesamtdauer:** 0.409 Sekunden  \n",
    "- **Ø  Zeit pro Vorhersage:** 0.014 Sekunden  \n",
    "- **Ø  Zeit pro Feature:** 0.003 Sekunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa9af6",
   "metadata": {},
   "source": [
    "# Idee für bessere Skalierbarkeit mit Dimensionen\n",
    "\n",
    "Divide and Conquer: Wenn mehr als 4 Features, daten in mehrere Tabellen aufteilen und in jeder Tabelle getrennt von einander Schießen. Anschließend die Trefferklassen kombinieren. ACHTUNG: wirkt sich auch auf potentiellen Bias aus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c021bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
