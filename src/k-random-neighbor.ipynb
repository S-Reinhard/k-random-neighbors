{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e202c5",
   "metadata": {},
   "source": [
    "# K-Random-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "76d8a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils.extmath import weighted_mode\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_openml\n",
    "from itertools import combinations\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8656f1",
   "metadata": {},
   "source": [
    "## K-Random-Neighbors klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03c8c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRandomNeighbors:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fit_map = None\n",
    "        self.stddev = None\n",
    "        self.X_shape = None\n",
    "        self.XTrain_view = None\n",
    "        self.stddev_target_scaler = 0.5\n",
    "        \n",
    "    def scaled_sigmoid(self, x, a, b, c):\n",
    "        return a / (1 + np.exp(c * (x - b)))\n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain):\n",
    "        # create a hashmap with feature tuples as keys and target values as values\n",
    "        self.fit_map = {tuple(x): y for x, y in zip(XTrain, YTrain)}\n",
    "        \n",
    "        # compute the standard deviation for each column in XTrain\n",
    "        self.stddev = np.std(XTrain, axis=0)\n",
    "        \n",
    "        # store the shape of the training data\n",
    "        self.X_shape = XTrain.shape\n",
    "        \n",
    "        # convert the training data of shape n*m into an ndarray of shape n*1 with tuple-like elements of form 1*m\n",
    "        self.XTrain_view = XTrain.view([('', XTrain.dtype)] * XTrain.shape[1])\n",
    "        \n",
    "        # TODO: optimize stddev_target_scaler\n",
    "        \n",
    "    def predict(self, X, k, num_points=1000):\n",
    "        hits = list()\n",
    "        \n",
    "        while len(hits) < k:\n",
    "            # generate data points\n",
    "            rng = np.random.default_rng()\n",
    "            points: np.ndarray = rng.normal(X, self.stddev * self.stddev_target_scaler, size=(num_points, self.X_shape[1])).round(1)\n",
    "            \n",
    "            # convert points into structured arrays (treat rows as tuples)\n",
    "            point_view = points.view([('', points.dtype)] * points.shape[1])\n",
    "            \n",
    "            # determine the intersection between generated points and training data\n",
    "            intersection: np.ndarray = np.intersect1d(self.XTrain_view, point_view)\n",
    "            # print(points[0, :], end='\\r', flush=True)\n",
    "            \n",
    "            if intersection.shape[0] > 0:\n",
    "                hits.extend(intersection.tolist())\n",
    "                \n",
    "        # determine class for each hit\n",
    "        classes = list()\n",
    "        for hit in hits:\n",
    "            classes.append(self.fit_map[hit])\n",
    "            \n",
    "        classes = np.array(classes)\n",
    "        \n",
    "        # TODO: assign weights for each hit, use scaled_sigmoid(), to accomplish this\n",
    "        weights = np.ones(shape=classes.shape)\n",
    "        \n",
    "        # determine the mode (most frequently occurring class)\n",
    "        prediction = weighted_mode(classes, weights)\n",
    "        \n",
    "        return prediction[0][0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7279b",
   "metadata": {},
   "source": [
    "# k-random-enseble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699be2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRandomEnsable:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.COMBINED_FEATURES:int = 4\n",
    "        self.WORKERS: list[KRandomEnsable] = list()\n",
    "        \n",
    "    def get_feature_combinations(self, dims):\n",
    "        # make sure that feature combinations are not longer than existing features\n",
    "        if (dims < self.COMBINED_FEATURES):\n",
    "            self.COMBINED_FEATURES = dims\n",
    "        elif (dims == self.COMBINED_FEATURES):\n",
    "            self.COMBINED_FEATURES -= 1\n",
    "        \n",
    "        # get idx of all features\n",
    "        feature_idxs = list(range(dims))\n",
    "        \n",
    "        # get all combinations of Features\n",
    "        return np.array(list(combinations(feature_idxs, self.COMBINED_FEATURES)))\n",
    "    \n",
    "    \n",
    "    def fit(self, XTrain: np.ndarray, YTrain):\n",
    "        # get possible combinations of features\n",
    "        combs = self.get_feature_combinations(XTrain.shape[1])\n",
    "        \n",
    "        # decide how many workers schould be used -> use as many combinations as there are cores, unless there are more cores than combinations\n",
    "        worker_count = min(multiprocessing.cpu_count(), combs.shape[0])\n",
    "        \n",
    "        # choose the combinations on which the workers are trained on\n",
    "        rng = np.random.default_rng()\n",
    "        comb_idxs = rng.choice(combs.shape[0], worker_count, replace=False)\n",
    "        combs = combs[comb_idxs]\n",
    "        \n",
    "        # train each worker with one combination\n",
    "        for current_combo in combs:            \n",
    "            # generate training dataset by using only the features at the given idxs in the current combination\n",
    "            X_subset = XTrain[:, current_combo]\n",
    "            \n",
    "            # create and Train worker\n",
    "            worker = KRandomEnsable()\n",
    "            worker.fit(X_subset, YTrain)\n",
    "            \n",
    "            # evaluate trusworthyness of the worker\n",
    "            \n",
    "            \n",
    "            self.WORKERS.append(worker)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406c9cc",
   "metadata": {},
   "source": [
    "## Test funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72ecb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn(XTrain: np.ndarray, YTrain: np.ndarray, XTest: np.ndarray, YTest: np.ndarray, k=30):\n",
    "    \"\"\"\n",
    "    Tests a k-NN model and prints confusion matrix as well as precision, recall, and F1-score.\n",
    "\n",
    "    Parameters:\n",
    "    - XTrain, YTrain: Training data\n",
    "    - XTest, YTest: Test data\n",
    "    - k: Number of neighbors\n",
    "    \"\"\"\n",
    "    # 1) Train the KNN\n",
    "    knn = KRandomNeighbors()\n",
    "    knn.fit(XTrain, YTrain)\n",
    "    \n",
    "    # Time measurement: Total time\n",
    "    start_total = time.time()\n",
    "\n",
    "    # 2) Predictions for all test samples\n",
    "    predictions = []\n",
    "    total_prediction_time = 0\n",
    "\n",
    "    for i in range(XTest.shape[0]):\n",
    "        start_pred = time.time()\n",
    "        pred = knn.predict(XTest[i], k)\n",
    "        end_pred = time.time()\n",
    "\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        pred_time = end_pred - start_pred\n",
    "        total_prediction_time += pred_time\n",
    "        # print(f\"Predicted class {pred} for {XTest[i]} in {round(pred_time, 3)} sec\")\n",
    "    \n",
    "    end_total = time.time()\n",
    "\n",
    "    # get all classes\n",
    "    classes = np.unique(np.concatenate([YTrain, YTest]))\n",
    "\n",
    "    # 3) Confusion matrix\n",
    "    cm = confusion_matrix(YTest, predictions, labels=classes)\n",
    "    # print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # 4) Precision, Recall, F1-score\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        YTest, predictions, labels=classes, zero_division=0\n",
    "    )\n",
    "\n",
    "    # print(\"\\nMetrics per class:\")\n",
    "    # for idx, label in enumerate(classes):\n",
    "    #     print(f\"Class {label}:\")\n",
    "    #     print(f\"  Precision: {precision[idx]:.2f}\")\n",
    "    #     print(f\"  Recall:    {recall[idx]:.2f}\")\n",
    "    #     print(f\"  F1-Score:  {f1[idx]:.2f}\")\n",
    "    #     print(f\"  Support:   {support[idx]} samples\\n\")\n",
    "        \n",
    "    # Average time per prediction\n",
    "    avg_prediction_time = total_prediction_time / XTest.shape[0]\n",
    "\n",
    "    # Time per feature\n",
    "    num_features = XTest.shape[1]\n",
    "    avg_time_per_feature = avg_prediction_time / num_features\n",
    "\n",
    "    total_time = round(end_total - start_total, 3)\n",
    "    time_per_prediction = round(avg_prediction_time, 3)\n",
    "    time_per_feature = round(avg_time_per_feature, 3)\n",
    "    # print(\"Total time:\", total_time, \"seconds\")\n",
    "    # print(\"Avg. time per prediction:\", time_per_prediction, \"seconds\")\n",
    "    # print(\"Avg. time per feature:\", time_per_feature, \"seconds\")\n",
    "\n",
    "    return cm, precision, recall, f1, support, total_time, time_per_prediction, time_per_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ad92e",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2bed15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2742acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numeric data as an ndarray\n",
    "dataset = np.loadtxt('iris_synthetic.csv', delimiter=',')\n",
    "# dataset = np.loadtxt('banknotes_numeric.csv', delimiter=',')\n",
    "\n",
    "# select features and target data\n",
    "X = dataset[:, :-1]  # all columns except the last\n",
    "Y = dataset[:, -1]   # last column is the class/label\n",
    "\n",
    "allIdx = np.arange(X.shape[0])  # all indices of the data\n",
    "\n",
    "# randomly choose 75% of the indices for training\n",
    "rng = np.random.default_rng(42)\n",
    "tests = dict()\n",
    "\n",
    "for i in range(30, 30, 30):\n",
    "    # train_idx = rng.choice(allIdx, size=int(allIdx.shape[0] * 0.2), replace=False)\n",
    "    train_idx = rng.choice(allIdx, size=int(i), replace=False)\n",
    "    test_idx = np.delete(allIdx, train_idx)  # remaining 20% for testing\n",
    "    test_idx = rng.choice(test_idx, size=100, replace=False)\n",
    "\n",
    "    XTrain = X[train_idx]\n",
    "    YTrain = Y[train_idx]\n",
    "\n",
    "    XTest = X[test_idx]\n",
    "    YTest = Y[test_idx]\n",
    "\n",
    "\n",
    "    # %lprun -f KRandomNeighbors.predict test_knn(XTrain, YTrain, XTest, YTest, k=11)\n",
    "\n",
    "    tests[i] = test_knn(XTrain, YTrain, XTest, YTest, k=11)\n",
    "    print(i, list(tests[i][-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d08687",
   "metadata": {},
   "source": [
    "### Auswertung Banknotes\n",
    "\n",
    "#### Confusion Matrix:\n",
    "|                          | Vorhergesagt: Klasse 1.0 | Vorhergesagt: Klasse 2.0 |\n",
    "|--------------------------|--------------------------|--------------------------|\n",
    "| Tatsächlich: Klasse 1.0  | 141                      | 11                       |\n",
    "| Tatsächlich: Klasse 2.0  | 0                        | 123                      |\n",
    "\n",
    "#### Metriken pro Klasse:\n",
    "Klasse 1.0:\n",
    "- Präzision: 1.00\n",
    "- Recall:    0.93\n",
    "- F1-Score:  0.96\n",
    "- Support:   152 Beispiele\n",
    "\n",
    "Klasse 2.0:\n",
    "- Präzision: 0.92\n",
    "- Recall:    1.00\n",
    "- F1-Score:  0.96\n",
    "- Support:   123 Beispiele\n",
    "\n",
    "#### Zeitmessungen:\n",
    "- Gesamtzeit: 295.178 Sekunden\n",
    "- Ø Zeit pro Vorhersage: 1.073 Sekunden\n",
    "- Ø Zeit pro Feature: 0.268 Sekunden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952c20c",
   "metadata": {},
   "source": [
    "# Auswertung Iris\n",
    "\n",
    "## Konfusionsmatrix\n",
    "\n",
    "|               | Vorhergesagt: Klasse 0 | Vorhergesagt: Klasse 1 | Vorhergesagt: Klasse 2 |\n",
    "|---------------|------------------------|------------------------|------------------------|\n",
    "| **Tatsächliche Klasse 0** | 11                     | 0                      | 0                      |\n",
    "| **Tatsächliche Klasse 1** | 0                      | 9                      | 2                      |\n",
    "| **Tatsächliche Klasse 2** | 0                      | 2                      | 6                      |\n",
    "\n",
    "\n",
    "\n",
    "## Metriken pro Klasse\n",
    "\n",
    "### Klasse 0.0\n",
    "- **Precision:** 1.00  \n",
    "- **Recall:** 1.00  \n",
    "- **F1-Score:** 1.00  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "### Klasse 1.0\n",
    "- **Precision:** 0.82  \n",
    "- **Recall:** 0.82  \n",
    "- **F1-Score:** 0.82  \n",
    "- **Support:** 11 Samples  \n",
    "\n",
    "### Klasse 2.0\n",
    "- **Precision:** 0.75  \n",
    "- **Recall:** 0.75  \n",
    "- **F1-Score:** 0.75  \n",
    "- **Support:** 8 Samples  \n",
    "\n",
    "## Zeitmessung\n",
    "\n",
    "- **Gesamtdauer:** 0.409 Sekunden  \n",
    "- **Ø  Zeit pro Vorhersage:** 0.014 Sekunden  \n",
    "- **Ø  Zeit pro Feature:** 0.003 Sekunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa9af6",
   "metadata": {},
   "source": [
    "# Idee für bessere Skalierbarkeit mit Dimensionen\n",
    "\n",
    "Divide and Conquer: Wenn mehr als 4 Features, daten in mehrere Tabellen aufteilen und in jeder Tabelle getrennt von einander Schießen. Anschließend die Trefferklassen kombinieren. ACHTUNG: wirkt sich auch auf potentiellen Bias aus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "147f41de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.9 3.1 4.3]\n",
      "[5.9 3.1 4.3 1.6]\n",
      "[5.9 3.1 1.6]\n",
      "[5.9 3.1 4.3 1.6]\n",
      "[3.1 4.3 1.6]\n",
      "[5.9 3.1 4.3 1.6]\n",
      "[5.9 4.3 1.6]\n",
      "[5.9 3.1 4.3 1.6]\n"
     ]
    }
   ],
   "source": [
    "g = KRandomEnsable()\n",
    "\n",
    "# train_idx = rng.choice(allIdx, size=int(allIdx.shape[0] * 0.2), replace=False)\n",
    "train_idx = rng.choice(allIdx, size=150, replace=False)\n",
    "test_idx = np.delete(allIdx, train_idx)  # remaining 20% for testing\n",
    "test_idx = rng.choice(test_idx, size=100, replace=False)\n",
    "\n",
    "XTrain = X[train_idx]\n",
    "YTrain = Y[train_idx]\n",
    "\n",
    "XTest = X[test_idx]\n",
    "YTest = Y[test_idx]\n",
    "\n",
    "\n",
    "g.fit(XTrain, YTrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
